{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2672a24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghcp/miniconda3/envs/haza_wokr_env/lib/python3.8/site-packages/zfit/__init__.py:63: UserWarning: TensorFlow warnings are by default suppressed by zfit. In order to show them, set the environment variable ZFIT_DISABLE_TF_WARNINGS=0. In order to suppress the TensorFlow warnings AND this warning, set ZFIT_DISABLE_TF_WARNINGS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.30/04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghcp/miniconda3/envs/haza_wokr_env/lib/python3.8/site-packages/zfit/core/basemodel.py:199: UserWarning: For the future, also decorate _pdf with @supports and specify what you support (such as 'norm=True' to keep the same behavior as before)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import zfit\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from iminuit import Minuit\n",
    "import warnings\n",
    "import os\n",
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "from PDFs import (\n",
    "    FullAngular_Physical_PDF, \n",
    "    FullAngular_Transformed_PDF, \n",
    "    get_inverse_values, \n",
    "    apply_transformation_equations,\n",
    "    get_physical_region_scan\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import cmsstyle\n",
    "import sys\n",
    "from plot_tools import create_axes_for_pulls, plot_model \n",
    "import plot_tools \n",
    "import customPDFs\n",
    "import mass_models\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "import mplhep as hep\n",
    "import os, sys\n",
    "import common_tools\n",
    "from scipy.stats import gaussian_kde\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import mplhep as hep\n",
    "hep.style.use(\"CMS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9f00bd",
   "metadata": {},
   "source": [
    "# Funciones importante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81caf983",
   "metadata": {},
   "outputs": [],
   "source": [
    "zfit.settings.set_seed(42)\n",
    "np.random.seed(42)\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "\n",
    "def calculate_jacobian_numerical(func, params, keys_out, epsilon=1e-5):\n",
    "    \"\"\"Calcula la matriz Jacobiana numéricamente.\"\"\"\n",
    "    n_params = len(params)\n",
    "    n_out = len(keys_out)\n",
    "    J = np.zeros((n_out, n_params))\n",
    "    \n",
    "    def func_wrapper(p_args):\n",
    "        res_dict = func(*p_args)\n",
    "        return np.array([res_dict[k] for k in keys_out])\n",
    "\n",
    "    for i in range(n_params):\n",
    "        p_plus = np.copy(params)\n",
    "        p_minus = np.copy(params)\n",
    "        p_plus[i] += epsilon\n",
    "        p_minus[i] -= epsilon\n",
    "        \n",
    "        f_plus = func_wrapper(p_plus)\n",
    "        f_minus = func_wrapper(p_minus)\n",
    "        \n",
    "        deriv = (f_plus - f_minus) / (2 * epsilon)\n",
    "        J[:, i] = deriv\n",
    "    return J\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c5dae3",
   "metadata": {},
   "source": [
    "# Generación de MC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e48a0e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Directorios listos:\n",
      "    - Plots/Transformed_Space_vFinal\n",
      "    - Plots/Physical_Space_Zoom_vFinal\n",
      "    - Plots/Physical_Space_Full_vFinal\n",
      ">>> Generando datos (Toy MC)...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#MAIN function \n",
    "folder_trans = \"Plots/Transformed_Space_vFinal\"\n",
    "folder_phys_zoom = \"Plots/Physical_Space_Zoom_vFinal\"\n",
    "folder_phys_full = \"Plots/Physical_Space_Full_vFinal\"\n",
    "\n",
    "for f in [folder_trans, folder_phys_zoom, folder_phys_full]:\n",
    "    os.makedirs(f, exist_ok=True)\n",
    "\n",
    "print(f\">>> Directorios listos:\\n    - {folder_trans}\\n    - {folder_phys_zoom}\\n    - {folder_phys_full}\")\n",
    "# GENERACIÓN (toys) ---\n",
    "obs = zfit.Space('cosThetaL', limits=(-1, 1)) * zfit.Space('cosThetaK', limits=(-1, 1)) * zfit.Space('phi', limits=(-np.pi, np.pi))\n",
    "\n",
    "# Valores verdaderos físicos lhcb\n",
    "true_vals_phys = [0.684, 0.014, 0.029, 0.050, -0.145, -0.136, -0.204, 0.077]\n",
    "phys_keys = ['FL', 'S3', 'S9', 'AFB', 'S4', 'S7', 'S5', 'S8']\n",
    "true_dict = dict(zip(phys_keys, true_vals_phys))\n",
    "\n",
    "print(\">>> Generando datos (Toy MC)...\")\n",
    "pdf_gen = FullAngular_Physical_PDF(obs, *true_vals_phys)\n",
    "sampler = pdf_gen.create_sampler(n=2000) \n",
    "sampler.resample()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e862caa",
   "metadata": {},
   "source": [
    "Fit in transformed space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a09364c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 2. Ejecutando Minimización (Minuit)...\n",
      ">>> 3. Calculando Errores MINOS...\n",
      "\n",
      "================================================================================\n",
      ">>> Errores MINOS (Espacio Transformado)\n",
      "================================================================================\n",
      "PARAM      | VALOR      | ERROR -    | ERROR +    | VERDAD    \n",
      "--------------------------------------------------------------------------------\n",
      "rFL        | 0.3667     | -0.0334     | +0.0346    | 0.3861\n",
      "rS3        | 0.0596     | -0.1200     | +0.1216    | 0.0888\n",
      "rS9        | 0.1201     | -0.1300     | +0.1391    | 0.1864\n",
      "rAFB       | 0.0773     | -0.0590     | +0.0571    | 0.2189\n",
      "rS4        | -0.5490     | -0.1600     | +0.1373    | -0.7812\n",
      "rS7        | -0.3659     | -0.0888     | +0.0734    | -0.4293\n",
      "rS5        | -0.3709     | -0.0594     | +0.0553    | -0.4484\n",
      "rS8        | 0.3803     | -0.1192     | +0.1356    | 0.3654\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      ">>> Errores Gaussianos (Matriz de Covarianza Propagada)\n",
      "================================================================================\n",
      "PARAM      | VALOR      | ERROR (+/-)     | VERDAD    \n",
      "--------------------------------------------------------------------------------\n",
      "FL         | 0.6755     | +/- 0.0149      | 0.6840\n",
      "S3         | 0.0097     | +/- 0.0194      | 0.0140\n",
      "S9         | 0.0193     | +/- 0.0213      | 0.0290\n",
      "AFB        | 0.0186     | +/- 0.0139      | 0.0500\n",
      "S4         | -0.1135     | +/- 0.0235      | -0.1450\n",
      "S7         | -0.1378     | +/- 0.0236      | -0.1360\n",
      "S5         | -0.1710     | +/- 0.0235      | -0.2040\n",
      "S8         | 0.0818     | +/- 0.0240      | 0.0770\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "raw_init = get_inverse_values(true_vals_phys)\n",
    "r_keys = ['rFL', 'rS3', 'rS9', 'rAFB', 'rS4', 'rS7', 'rS5', 'rS8']\n",
    "param_names_fit = [f\"{k}_fit\" for k in r_keys]\n",
    "\n",
    "true_vals_trans_dict = dict(zip(param_names_fit, raw_init))\n",
    "\n",
    "params = {k: zfit.Parameter(p_name, v, step_size=0.01) \n",
    "            for k, p_name, v in zip(r_keys, param_names_fit, raw_init)}\n",
    "\n",
    "pdf_fit = FullAngular_Transformed_PDF(obs, params['rFL'], params['rS3'], params['rS9'], params['rAFB'],params['rS4'], params['rS7'], params['rS5'], params['rS8'])\n",
    "\n",
    "print(\">>> 2. Ejecutando Minimización (Minuit)...\")\n",
    "nll = zfit.loss.UnbinnedNLL(model=pdf_fit, data=sampler)\n",
    "minimizer = zfit.minimize.Minuit(tol=0.01) \n",
    "result = minimizer.minimize(nll)\n",
    "m = result.info['minuit'] \n",
    "\n",
    "print(\">>> 3. Calculando Errores MINOS...\")\n",
    "m.minos() \n",
    "\n",
    "# =======================================\n",
    "# ESPACIO TRANSFORMADO (VALORES DE MINUIT)\n",
    "# =======================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\">>> Errores MINOS (Espacio Transformado)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'PARAM':<10} | {'VALOR':<10} | {'ERROR -':<10} | {'ERROR +':<10} | {'VERDAD':<10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Aquí guarda los valores CORRECTOS de Minuit para usarlos después\n",
    "minuit_best_values = []\n",
    "\n",
    "for rk, pname in zip(r_keys, param_names_fit):\n",
    "    val = m.values[pname]\n",
    "    minuit_best_values.append(val) # guarda el valor exacto de Minuit\n",
    "    \n",
    "    err_low = m.merrors[pname].lower  \n",
    "    err_high = m.merrors[pname].upper \n",
    "    truth = true_vals_trans_dict[pname]\n",
    "    \n",
    "    print(f\"{rk:<10} | {val:.4f}     | {err_low:.4f}     | +{err_high:.4f}    | {truth:.4f}\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# ===========================================\n",
    "# ESPACIO FÍSICO (CORREGIDO PARA USAR MINUIT)\n",
    "# ===========================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\">>> Errores Gaussianos (Matriz de Covarianza Propagada)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# recupera la matriz de covarianza\n",
    "param_objs_ordered = [params[k] for k in r_keys]\n",
    "cov_trans = result.covariance(params=param_objs_ordered)\n",
    "\n",
    "# IMPORTANTE usar los valores de MINUIT, no los de zfit.Parameter (revisar pq)\n",
    "best_fit_r_values = np.array(minuit_best_values)\n",
    "\n",
    "# Jacobiano y Propagación\n",
    "J = calculate_jacobian_numerical(apply_transformation_equations, best_fit_r_values, phys_keys)\n",
    "cov_phys = J @ cov_trans @ J.T\n",
    "phys_errors_sigma = np.sqrt(np.diag(cov_phys))\n",
    "phys_errors_dict = dict(zip(phys_keys, phys_errors_sigma))\n",
    "\n",
    "# 4. Tabla (Calculada con los valores de Minuit)\n",
    "print(f\"{'PARAM':<10} | {'VALOR':<10} | {'ERROR (+/-)':<15} | {'VERDAD':<10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "best_fit_phys_dict = apply_transformation_equations(*best_fit_r_values)\n",
    "\n",
    "for k in phys_keys:\n",
    "    val = best_fit_phys_dict[k]\n",
    "    err = phys_errors_dict[k]\n",
    "    tru = true_dict[k]\n",
    "    print(f\"{k:<10} | {val:.4f}     | +/- {err:.4f}      | {tru:.4f}\")\n",
    "print(\"=\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00d1ab4",
   "metadata": {},
   "source": [
    "# Contornos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "724118df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [28/28] S5 vs S8 ....\r"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#GENERACIÓN DEL MAPA FÍSICO ---\n",
    "df_phys_region = get_physical_region_scan(n_points=1000)\n",
    "indices = list(range(8))\n",
    "pairs_indices = list(combinations(indices, 2))\n",
    "total_plots = len(pairs_indices)\n",
    "\n",
    "for i, (idx_x, idx_y) in enumerate(pairs_indices):\n",
    "    \n",
    "    rx, ry = param_names_fit[idx_x], param_names_fit[idx_y]\n",
    "    px, py = phys_keys[idx_x], phys_keys[idx_y]\n",
    "\n",
    "    print(f\"    [{i+1}/{total_plots}] {px} vs {py} ...\", end=\"\\r\")\n",
    "\n",
    "    # 1. OBTENER CONTORNO (Espacio R)\n",
    "    contour_r = m.mncontour(rx, ry, cl=0.3935, size=50)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # PLOT A: Espacio Transformado\n",
    "    # ---------------------------------------------------------\n",
    "    plt.figure(figsize=(8, 7))\n",
    "    plt.plot(contour_r[:, 0], contour_r[:, 1], 'b-', linewidth=2, label='1$\\sigma$ Contour')\n",
    "    plt.plot(m.values[rx], m.values[ry], 'bo', label='Best Fit', zorder=10)\n",
    "    true_tx = true_vals_trans_dict[rx]\n",
    "    true_ty = true_vals_trans_dict[ry]\n",
    "    plt.plot(true_tx, true_ty, 'r*', markersize=14, markeredgecolor='k', label='True Value', zorder=15)\n",
    "\n",
    "    # MINOS Errors visuales\n",
    "    val_x = m.values[rx]; val_y = m.values[ry]\n",
    "    minos_x_low = val_x + m.merrors[rx].lower; minos_x_high = val_x + m.merrors[rx].upper\n",
    "    minos_y_low = val_y + m.merrors[ry].lower; minos_y_high = val_y + m.merrors[ry].upper\n",
    "    \n",
    "    plt.axvline(minos_x_low, color='k', linestyle='--', alpha=0.4)\n",
    "    plt.axvline(minos_x_high, color='k', linestyle='--', alpha=0.4)\n",
    "    plt.axhline(minos_y_low, color='k', linestyle='--', alpha=0.4)\n",
    "    plt.axhline(minos_y_high, color='k', linestyle='--', alpha=0.4, label='MINOS Errors')\n",
    "    \n",
    "    width_x = minos_x_high - minos_x_low; width_y = minos_y_high - minos_y_low\n",
    "    margin = 0.4\n",
    "    plt.xlim(minos_x_low - width_x * margin, minos_x_high + width_x * margin)\n",
    "    plt.ylim(minos_y_low - width_y * margin, minos_y_high + width_y * margin)\n",
    "\n",
    "    plt.xlabel(rx); plt.ylabel(ry)\n",
    "    plt.title(f\"Transformed: {rx} vs {ry}\")\n",
    "    plt.grid(True, alpha=0.3); plt.legend()\n",
    "    plt.savefig(f\"{folder_trans}/Trans_{rx}_vs_{ry}.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # -------------\n",
    "    # PLOTS FÍSICOS\n",
    "    # -------------\n",
    "    n_pts = len(contour_r)\n",
    "    r_matrix = np.tile(best_fit_r_values, (n_pts, 1))\n",
    "    r_matrix[:, idx_x] = contour_r[:, 0]\n",
    "    r_matrix[:, idx_y] = contour_r[:, 1]\n",
    "    trans_contour_dict = apply_transformation_equations(r_matrix[:, 0], r_matrix[:, 1], r_matrix[:, 2], r_matrix[:, 3],r_matrix[:, 4], r_matrix[:, 5], r_matrix[:, 6], r_matrix[:, 7])\n",
    "    cx_phys = trans_contour_dict[px]\n",
    "    cy_phys = trans_contour_dict[py]\n",
    "\n",
    "    # CÁLCULO DE ERRORES GEOMÉTRICOS\n",
    "    x_min_cont = np.min(cx_phys)\n",
    "    x_max_cont = np.max(cx_phys)\n",
    "    y_min_cont = np.min(cy_phys)\n",
    "    y_max_cont = np.max(cy_phys)\n",
    "    \n",
    "    bf_x = best_fit_phys_dict[px]\n",
    "    bf_y = best_fit_phys_dict[py]\n",
    "    \n",
    "    err_x_up = x_max_cont - bf_x\n",
    "    err_x_down = bf_x - x_min_cont\n",
    "    err_y_up = y_max_cont - bf_y\n",
    "    err_y_down = bf_y - y_min_cont\n",
    "\n",
    "    def plot_physical(view_mode, x_lims=None, y_lims=None):\n",
    "        plt.figure(figsize=(9, 8))\n",
    "        alpha_cloud = 0.15 if view_mode == 'Full' else 0.05\n",
    "        \n",
    "        # plt.scatter(df_phys_region[px], df_phys_region[py],c='gray', s=1, alpha=alpha_cloud, label='Allowed Region', zorder=0)\n",
    "        \n",
    "        plt.plot(cx_phys, cy_phys, 'g-', linewidth=2.5, label='1$\\sigma$ Contour')\n",
    "        plt.plot(bf_x, bf_y, 'bo', markersize=6, label='Best Fit', zorder=10)\n",
    "        plt.plot(true_dict[px], true_dict[py], 'r*', markersize=14, markeredgecolor='k', label='True Value', zorder=11)\n",
    "        \n",
    "        plt.axvline(x_min_cont, color='red', linestyle='--', linewidth=1, alpha=0.7)\n",
    "        plt.axvline(x_max_cont, color='red', linestyle='--', linewidth=1, alpha=0.7)\n",
    "        plt.axhline(y_min_cont, color='red', linestyle='--', linewidth=1, alpha=0.7)\n",
    "        plt.axhline(y_max_cont, color='red', linestyle='--', linewidth=1, alpha=0.7, label='Contour Limits')\n",
    "\n",
    "        plt.xlabel(px, fontsize=14); plt.ylabel(py, fontsize=14)\n",
    "        \n",
    "        if view_mode == 'Zoom':\n",
    "            title_str = (f\"{px} = {bf_x:.4f} (+{err_x_up:.4f}/-{err_x_down:.4f})\\n\"\n",
    "                            f\"{py} = {bf_y:.4f} (+{err_y_up:.4f}/-{err_y_down:.4f})\")\n",
    "            plt.title(title_str, fontsize=11, color='blue')\n",
    "            if x_lims: plt.xlim(x_lims); \n",
    "            if y_lims: plt.ylim(y_lims)\n",
    "        else:\n",
    "            plt.title(f\"FULL: {px} vs {py}\", fontsize=12)\n",
    "            plt.xlim(df_phys_region[px].min(), df_phys_region[px].max())\n",
    "            plt.ylim(df_phys_region[py].min(), df_phys_region[py].max())\n",
    "\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend(loc='upper right', frameon=True, fontsize=9)\n",
    "        \n",
    "        folder = folder_phys_zoom if view_mode == 'Zoom' else folder_phys_full\n",
    "        plt.savefig(f\"{folder}/Phys_{px}_vs_{py}_{view_mode}.png\", dpi=100)\n",
    "        plt.close()\n",
    "\n",
    "    # Definir límites de zoom\n",
    "    mx = (x_max_cont - x_min_cont) * 0.4\n",
    "    my = (y_max_cont - y_min_cont) * 0.4\n",
    "    \n",
    "    plot_physical('Zoom', x_lims=(x_min_cont-mx, x_max_cont+mx), y_lims=(y_min_cont-my, y_max_cont+my))\n",
    "    plot_physical('Full')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5b30ae",
   "metadata": {},
   "source": [
    "# Proyecciones v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e99f427a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mplhep as hep\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from scipy.stats import chi2 as _chi2\n",
    "from scipy.integrate import quad\n",
    "\n",
    "\n",
    "def analytic_proj_cosK(x, FL):\n",
    "    \"\"\"\n",
    "    Densidad normalizada en [-1,1] para cosThetaK (fallback).\n",
    "    \"\"\"\n",
    "    x = np.asarray(x)\n",
    "    vals = 0.75 * (1.0 - FL) * (1.0 - x**2) + 1.5 * FL * x**2\n",
    "    # normalizar numéricamente\n",
    "    norm = np.trapz(vals, x) if x.size > 1 else None\n",
    "    return vals if norm is None else vals / (norm + 1e-12)\n",
    "\n",
    "def analytic_proj_cosL(x, FL, AFB):\n",
    "    \"\"\"\n",
    "    Densidad normalizada en [-1,1] para cosThetaL (fallback).\n",
    "    f(x) = 3/8 (1+FL) + 3/8 (1-3FL) x^2 + AFB * x\n",
    "    \"\"\"\n",
    "    x = np.asarray(x)\n",
    "    vals = (3.0/8.0) * (1.0 + FL) + (3.0/8.0) * (1.0 - 3.0 * FL) * x**2 + AFB * x\n",
    "    # normalizamos numéricamente si es vector\n",
    "    if x.size > 1:\n",
    "        norm = np.trapz(vals, x)\n",
    "        return vals / (norm + 1e-12)\n",
    "    return vals\n",
    "\n",
    "def analytic_proj_phi(phi, S3=0.0, S9=0.0, FL=None):\n",
    "    \"\"\"\n",
    "    Fallback para phi. Si no conocemos la forma exacta, usamos la uniforme 1/(2π)\n",
    "    y añadimos modulaciones cos(2φ), sin(2φ) con amplitudes S3,S9 escaladas de forma conservadora.\n",
    "    \"\"\"\n",
    "    phi = np.asarray(phi)\n",
    "    base = np.ones_like(phi) / (2.0 * np.pi)\n",
    "    # mod = (4.0/3.0) * (S3 * np.cos(2.0 * phi) + S9 * np.sin(2.0 * phi))\n",
    "    # vals = base * (1.0 + mod)\n",
    "    mod = S3 * np.cos(2.0 * phi) + S9 * np.sin(2.0 * phi)\n",
    "    vals = base * (1.0 + mod)\n",
    "    if phi.size > 1:\n",
    "        norm = np.trapz(vals, phi)\n",
    "        return vals / (norm + 1e-12)\n",
    "    return vals\n",
    "\n",
    "hep.style.use(\"CMS\")\n",
    "\n",
    "def _sampler_to_numpy(sampler):\n",
    "    arr = np.asarray(sampler)\n",
    "    if arr.ndim == 2 and arr.shape[1] == 3:\n",
    "        return arr\n",
    "\n",
    "\n",
    "    \n",
    "    if hasattr(sampler, \"numpy\"):\n",
    "        arr = sampler.numpy()\n",
    "        arr = np.asarray(arr)\n",
    "        if arr.ndim == 2 and arr.shape[1] == 3:\n",
    "            return arr\n",
    "        # a veces sampler.numpy() devuelve (3,N)\n",
    "        if arr.ndim == 2 and arr.shape[0] == 3:\n",
    "            return arr.T\n",
    "\n",
    "\n",
    "    if hasattr(sampler, \"resample\"):\n",
    "        out = sampler.resample()\n",
    "        arr = np.asarray(out)\n",
    "        if arr.ndim == 2 and arr.shape[1] == 3:\n",
    "            return arr\n",
    "    if hasattr(sampler, \"sample\"):\n",
    "        out = sampler.sample()\n",
    "        arr = np.asarray(out)\n",
    "        if arr.ndim == 2 and arr.shape[1] == 3:\n",
    "            return arr\n",
    "\n",
    "\n",
    "def _integrate_pdf_on_bin(func, a, b, args=(), n_quad_points=50):\n",
    "    val, err = quad(lambda xx: func(xx, *args), a, b, epsabs=1e-8, epsrel=1e-6, limit=100)\n",
    "    return val\n",
    "\n",
    "def _rebin_until_expected_ok(edges, counts, expected, min_expected):\n",
    "    \"\"\"\n",
    "    Rebin simple: iteramos bins de izquierda a derecha acumulando hasta que expected_acc >= min_expected,\n",
    "    luego cerramos un bin combinado y seguimos.\n",
    "    Devuelve new_edges, new_counts, new_expected.\n",
    "    \"\"\"\n",
    "    new_edges = []\n",
    "    new_counts = []\n",
    "    new_expected = []\n",
    "\n",
    "    cur_left = edges[0]\n",
    "    cur_counts = 0.0\n",
    "    cur_expected = 0.0\n",
    "\n",
    "    for i in range(len(expected)):\n",
    "        cur_right = edges[i+1]\n",
    "        cur_counts += counts[i]\n",
    "        cur_expected += expected[i]\n",
    "\n",
    "        if cur_expected >= min_expected or i == len(expected) - 1:\n",
    "            new_edges.append(cur_left)\n",
    "            new_edges.append(cur_right)\n",
    "            new_counts.append(cur_counts)\n",
    "            new_expected.append(cur_expected)\n",
    "            # reset\n",
    "            if i != len(expected) - 1:\n",
    "                cur_left = cur_right\n",
    "                cur_counts = 0.0\n",
    "                cur_expected = 0.0\n",
    "\n",
    "    # transformar new_edges a array de edges ordenadas (puede haber duplicados)\n",
    "    # new_edges actualmente [left0,right0,left1,right1,...], convertimos a monotónica\n",
    "    e = [new_edges[0]]\n",
    "    for i in range(0, len(new_edges), 2):\n",
    "        right_i = new_edges[i+1]\n",
    "        e.append(right_i)\n",
    "    e = np.array(e)\n",
    "    return e, np.array(new_counts), np.array(new_expected)\n",
    "\n",
    "def plot_analytical_projections(sampler, phys_params_dict, folder_out, n_bins=40, min_expected=5.0, rebin_if_needed=True):\n",
    "    \"\"\"\n",
    "    Genera proyecciones 1D (cosThetaK, cosThetaL, phi) comparando toy (sampler) con\n",
    "    la PDF analítica construida a partir de phys_params_dict.\n",
    "    Guarda PNGs en folder_out y devuelve un diccionario con estadísticas de ajuste.\n",
    "    - sampler: objeto que contiene los eventos (columnas: cosL, cosK, phi)\n",
    "    - phys_params_dict: dict con claves mínimo: 'FL','AFB','S3','S9' (otras son ignoradas)\n",
    "    \"\"\"\n",
    "    os.makedirs(folder_out, exist_ok=True)\n",
    "\n",
    "    data = _sampler_to_numpy(sampler)\n",
    "    # asumimos columnas [cosL, cosK, phi]\n",
    "    cosL = data[:, 0]\n",
    "    cosK = data[:, 1]\n",
    "    phi  = data[:, 2]\n",
    "    n_events = len(cosL)\n",
    "\n",
    "    # parametros (fallback a 0 si no están)\n",
    "    FL  = float(phys_params_dict.get('FL', 0.0))\n",
    "    AFB = float(phys_params_dict.get('AFB', 0.0))\n",
    "    S3  = float(phys_params_dict.get('S3', 0.0))\n",
    "    S9  = float(phys_params_dict.get('S9', 0.0))\n",
    "\n",
    "    # seleccion de funciones analíticas (usar PDFs.py si está)\n",
    "    f_cosK = lambda x: analytic_proj_cosK(x, FL)\n",
    "    f_cosL = lambda x: analytic_proj_cosL(x, FL, AFB)\n",
    "    f_phi  = lambda x: analytic_proj_phi(x, S3, S9, FL)\n",
    "\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    vars_cfg = [\n",
    "        (cosK, f_cosK, r'$\\cos\\theta_K$', 'CosThetaK', (-1.0, 1.0), 1),   # params_effective=1 (FL)\n",
    "        (cosL, f_cosL, r'$\\cos\\theta_L$', 'CosThetaL', (-1.0, 1.0), 2),   # params_effective=2 (FL, AFB)\n",
    "        (phi,  f_phi,  r'$\\phi$',         'Phi',       (-np.pi, np.pi), 2) # params_effective approx=2 (S3,S9)\n",
    "    ]\n",
    "\n",
    "    for arr, func, xlabel, name, limits, params_effective in vars_cfg:\n",
    "        edges = np.linspace(limits[0], limits[1], n_bins + 1)\n",
    "        counts, _ = np.histogram(arr, bins=edges)\n",
    "        centers = 0.5 * (edges[:-1] + edges[1:])\n",
    "        bin_widths = edges[1:] - edges[:-1]\n",
    "\n",
    "        expected = np.zeros_like(counts, dtype=float)\n",
    "        for i in range(len(expected)):\n",
    "            a, b = edges[i], edges[i+1]\n",
    "            # si es phi y el bin cruza el -pi/pi, ajustamos para integrar correctamente (no necesario si edges exactos)\n",
    "            expected[i] = _integrate_pdf_on_bin(func, a, b) * n_events\n",
    "\n",
    "        # rebin si hay bins esperados pequeños\n",
    "        if rebin_if_needed and np.any(expected < min_expected):\n",
    "            edges_new, counts_new, expected_new = _rebin_until_expected_ok(edges, counts, expected, min_expected)\n",
    "            # recompute centers and widths\n",
    "            edges = edges_new\n",
    "            counts = counts_new\n",
    "            expected = expected_new\n",
    "            centers = 0.5 * (edges[:-1] + edges[1:])\n",
    "            bin_widths = edges[1:] - edges[:-1]\n",
    "\n",
    "        # evitar ceros exactos\n",
    "        eps = 1e-9\n",
    "        expected_safe = np.where(expected <= 0, eps, expected)\n",
    "\n",
    "        # pulls y chi2 (usar E en denominador - prueba de Pearson)\n",
    "        pulls = (counts - expected) / np.sqrt(expected_safe)\n",
    "        mask_valid = expected > 0  # bins utilizados\n",
    "        chi2_val = np.sum(((counts[mask_valid] - expected[mask_valid])**2) / expected_safe[mask_valid])\n",
    "\n",
    "        n_bins_used = mask_valid.sum()\n",
    "        ndof = max(1, int(n_bins_used) - int(params_effective))\n",
    "        p_val = _chi2.sf(chi2_val, ndof)\n",
    "        chi2_red = chi2_val / ndof\n",
    "\n",
    "        # para la curva continua (conteos por bin) evaluamos f(x) en grilla fina y convertimos a counts/bin\n",
    "        x_fine = np.linspace(limits[0], limits[1], 2000)\n",
    "        f_fine = func(x_fine)\n",
    "        # asegurar que f_fine está normalizada a densidad (integral 1) — si func ya devuelve densidad esto no cambia\n",
    "        integral_f = np.trapz(f_fine, x_fine)\n",
    "        if integral_f <= 0:\n",
    "            integral_f = 1.0\n",
    "        f_fine_norm = f_fine / integral_f\n",
    "        y_smooth_counts = f_fine_norm * n_events * ( (edges[1] - edges[0]) ) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #======\n",
    "        # plot\n",
    "        #======\n",
    "        fig = plt.figure(figsize=(9, 9))\n",
    "        gs = GridSpec(2, 1, height_ratios=[3.5, 1], hspace=0.08)\n",
    "        ax0 = fig.add_subplot(gs[0])\n",
    "        ax1 = fig.add_subplot(gs[1])\n",
    "\n",
    "        # datos: puntos con error (sqrt(O))\n",
    "        y_err_data = np.sqrt(counts)\n",
    "        ax0.errorbar(centers, counts, yerr=y_err_data, xerr=bin_widths/2.0, fmt='ks', markersize=4, elinewidth=1, capsize=2, label='Toy (data)')\n",
    "\n",
    "        # curva continua (suponemos bins uniformes iniciales para escala visual). Si rebin cambió bin widths, mostramos también los puntos esperados por bin\n",
    "        ax0.plot(x_fine, y_smooth_counts, '-', linewidth=1, label='Analytical PDF', color='b')\n",
    "        max_data = np.max(counts + y_err_data) # Altura del bin más alto + su barra de error\n",
    "        max_model = np.max(y_smooth_counts)    # Altura máxima de la curva azul\n",
    "        y_max_plot = max(max_data, max_model)\n",
    "        # 3. Añadimos un margen del 40% (multiplicar por 1.4) para que quepa el cuadro de texto\n",
    "        ax0.set_ylim(0, y_max_plot * 1.5) \n",
    "\n",
    "        ax0.set_ylabel(f'Events / bin', fontsize=14)\n",
    "        ax0.set_xlim(limits)\n",
    "        ax0.set_xticklabels([])\n",
    "        ax0.set_ylabel(f'Events / bin', fontsize=14)\n",
    "        ax0.set_xlim(limits)\n",
    "        ax0.set_xticklabels([])\n",
    "        ax0.grid(True, alpha=0.2)\n",
    "        ax0.legend(loc='best', fontsize=12, frameon=False)\n",
    "\n",
    "        hep.cms.label(data=False, loc=0, ax=ax0, rlabel=\"13 TeV\")\n",
    "\n",
    "        # anotación estadística\n",
    "        param_text = (\n",
    "            rf'$F_L = {FL:.3f}$' + '\\n' +\n",
    "            rf'$A_{{FB}} = {AFB:.3f}$' + '\\n' +\n",
    "            rf'$S_3 = {S3:.3f}$' + '\\n' +\n",
    "            rf'$S_9 = {S9:.3f}$' + '\\n' +\n",
    "            rf'$\\chi^2/\\mathrm{{ndof}} = {chi2_val:.2f}/{ndof} = {chi2_red:.2f}$' + '\\n' +\n",
    "            rf'$p$-value = {p_val:.3g}'\n",
    "        )\n",
    "\n",
    "        ax0.text(0.05, 0.95, param_text, transform=ax0.transAxes, fontsize=11, verticalalignment='top',bbox=dict( facecolor='white', alpha=0.01))\n",
    "\n",
    "        # panel de pulls\n",
    "        ax1.errorbar(centers, pulls, \n",
    "                     yerr=1.0,           # El error del pull normalizado es siempre 1\n",
    "                     xerr=0,             # Generalmente no se pone error en X en los pulls para limpieza\n",
    "                     fmt='ks',           # 'k'=negro, 'o'=círculo\n",
    "                     markersize=1,       # Equivalente aproximado a s=22\n",
    "                     elinewidth=1.0,     # Grosor de la barra vertical\n",
    "                     capsize=0)          \n",
    "        # Línea de referencia en 0\n",
    "        ax1.axhline(0, color='black', linewidth=1.0, linestyle='-')\n",
    "        ax1.axhline(0, color='black', linestyle='', linewidth=1, alpha=0.4)\n",
    "        ax1.axhline(3, color='black', linestyle=':', linewidth=1, alpha=0.8, label=r'3$\\sigma$')\n",
    "        ax1.axhline(-3, color='black', linestyle=':', linewidth=1, alpha=0.8)\n",
    "        ax1.set_xlabel(xlabel, fontsize=14)\n",
    "        ax1.set_ylabel(r'Pull', fontsize=11)\n",
    "        ax1.set_xlim(limits)\n",
    "        ax1.set_ylim(-5.0, 5.0)\n",
    "        ax1.grid(True, alpha=0.2)\n",
    "\n",
    "        ax0.tick_params(axis='both', which='major', labelsize=14) \n",
    "        ax1.tick_params(axis='both', which='major', labelsize=14)\n",
    "        # guardar\n",
    "        outname = os.path.join(folder_out, f\"CMS_Proj_{name}_Chi2.png\")\n",
    "        plt.savefig(outname, bbox_inches='tight', dpi=150)\n",
    "        plt.close(fig)\n",
    "\n",
    "        # guardar resultados\n",
    "        results[name] = {\n",
    "            'counts': counts,\n",
    "            'edges': edges,\n",
    "            'expected': expected,\n",
    "            'pulls': pulls,\n",
    "            'chi2': float(chi2_val),\n",
    "            'ndof': int(ndof),\n",
    "            'chi2_red': float(chi2_red),\n",
    "            'p_value': float(p_val),\n",
    "            'n_events': int(n_events),\n",
    "            'params_used': params_effective,\n",
    "            'plot_file': outname\n",
    "        }\n",
    "\n",
    "        print(f\"[OK] {name}: χ2/ndof = {chi2_val:.2f}/{ndof} ({chi2_red:.2f}), p = {p_val:.3g}  — plot -> {outname}\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9668120b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] CosThetaK: χ2/ndof = 22.36/29 (0.77), p = 0.805  — plot -> Plots/Analytic_Projections/CMS_Proj_CosThetaK_Chi2.png\n",
      "[OK] CosThetaL: χ2/ndof = 20.82/28 (0.74), p = 0.833  — plot -> Plots/Analytic_Projections/CMS_Proj_CosThetaL_Chi2.png\n",
      "[OK] Phi: χ2/ndof = 17.93/28 (0.64), p = 0.928  — plot -> Plots/Analytic_Projections/CMS_Proj_Phi_Chi2.png\n",
      ">>> Proyecciones listas.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results_proj = plot_analytical_projections(\n",
    "    sampler=sampler,\n",
    "    phys_params_dict=best_fit_phys_dict,\n",
    "    folder_out=\"Plots/Analytic_Projections\",\n",
    "    n_bins=30,           \n",
    "    min_expected=5.0,   \n",
    "    rebin_if_needed=True\n",
    ")\n",
    "\n",
    "print(\">>> Proyecciones listas.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfda27d",
   "metadata": {},
   "source": [
    "# Allowed region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716db1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GENERACIÓN DEL MAPA FÍSICO ---\n",
    "df_phys_region = get_physical_region_scan(n_points=10000)\n",
    "indices = list(range(8))\n",
    "pairs_indices = list(combinations(indices, 2))\n",
    "total_plots = len(pairs_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22534ea1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbbef0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize, Bounds\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "import math\n",
    "import time\n",
    "\n",
    "def constraints(params):\n",
    "    \"\"\"\n",
    "    Mapeo explícito de las 5 Desigualdades Físicas.\n",
    "    params = [FL, S3, S9, AFB, S4, S7, S5, S8]\n",
    "    \"\"\"\n",
    "    FL, S3, S9, AFB, S4, S7, S5, S8 = params\n",
    "    \n",
    "    # --- Desigualdad 1: 0 <= FL <= 1 ---\n",
    "    # Nota: Aunque esta se suele manejar en 'bounds', si queremos ser\n",
    "    # rigurosos en esta función, podemos retornarla, aunque el optimizador\n",
    "    # ya la fuerce en los límites de caja.\n",
    "    # Se desdobla en dos: FL >= 0 y 1 - FL >= 0\n",
    "    # Pero para el conteo de desigualdades complejas, nos enfocamos en las acopladas:\n",
    "    \n",
    "    # |S3| <= 0.5 * (1 - FL) ---\n",
    "    # Matemáticamente equivale a: (0.5*(1-FL))^2 - S3^2 >= 0\n",
    "    c2 = 0.25 * (1 - FL)**2 - S3**2\n",
    "    \n",
    "    # S3^2 + 4/9 AFB^2 + S9^2 <= 1/4 (1 - FL)^2 ---\n",
    "    # Pasamos todo a un lado: RHS - LHS >= 0\n",
    "    c3 = 0.25 * (1 - FL)**2 - (S3**2 + (4.0/9.0)*AFB**2 + S9**2)\n",
    "    \n",
    "    # 4 S4^2 + S7^2 <= FL (1 - FL - 2 S3) ---\n",
    "    # RHS - LHS >= 0\n",
    "    c4 = FL * (1 - FL - 2*S3) - (4*S4**2 + S7**2)\n",
    "    \n",
    "    # S5^2 + 4 S8^2 <= FL (1 - FL + 2 S3) ---\n",
    "    # RHS - LHS >= 0\n",
    "    c5 = FL * (1 - FL + 2*S3) - (S5**2 + 4*S8**2)\n",
    "    \n",
    "    return np.array([c2, c3, c4, c5])\n",
    "\n",
    "def get_profiled_boundary(x_idx, y_idx, n_steps=60):\n",
    "    \n",
    "    # 1. PASO NUEVO: Encontrar los límites REALES de X permitidos por la física\n",
    "    # En lugar de usar rangos fijos, preguntamos al optimizador hasta dónde llega X.\n",
    "    \n",
    "    def objective_min_x(p): return p[x_idx]\n",
    "    def objective_max_x(p): return -p[x_idx]\n",
    "    def phys_constraints(p): return constraints(p)\n",
    "    \n",
    "    cons = [{'type': 'ineq', 'fun': phys_constraints}]\n",
    "    bounds_opt = [(0, 1)] + [(-1, 1)]*7\n",
    "    \n",
    "    # Semilla central segura\n",
    "    x0 = np.array([0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "    \n",
    "    # Buscamos X_min real\n",
    "    res_xmin = minimize(objective_min_x, x0, method='SLSQP', bounds=bounds_opt, constraints=cons, tol=1e-6)\n",
    "    real_start = res_xmin.fun if res_xmin.success else 0.0\n",
    "    \n",
    "    # Buscamos X_max real\n",
    "    res_xmax = minimize(objective_max_x, x0, method='SLSQP', bounds=bounds_opt, constraints=cons, tol=1e-6)\n",
    "    real_end = -res_xmax.fun if res_xmax.success else 1.0\n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # 2. Ahora escaneamos EXACTAMENTE entre esos límites encontrados\n",
    "    # Usamos un margen epsilon mucho más pequeño, solo para estabilidad\n",
    "    epsilon = 1e-5 \n",
    "    x_vals = np.linspace(real_start + epsilon, real_end - epsilon, n_steps)\n",
    "    \n",
    "    y_min_vals = []\n",
    "    y_max_vals = []\n",
    "    valid_x = []\n",
    "    bounds = [(0, 1)] + [(-1, 1)]*7\n",
    "    # Actual (puede fallar en regiones complejas)\n",
    "    base_guesses = [...]\n",
    "\n",
    "    # # Sugerencia: Añadir semillas en extremos de parámetros\n",
    "    # base_guesses = [\n",
    "    #     # Centro\n",
    "    #     np.array([0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]),\n",
    "    #     # Extremos de FL\n",
    "    #     np.array([0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]),\n",
    "    #     np.array([0.99, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]),\n",
    "    #     # Combinaciones de signos para parámetros acoplados\n",
    "    #     np.array([0.3, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]),\n",
    "    #     np.array([0.3, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1]),\n",
    "    #     # Punto con FL=0.5, S3 máximo/minimo permitido\n",
    "    #     np.array([0.5, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]),\n",
    "    #     np.array([0.5, -0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]),\n",
    "    # ]\n",
    "    # --- SEMILLAS ---\n",
    "    base_guesses = [\n",
    "        np.array([0.5,  0.0, 0,0,0,0,0,0]), # Centro\n",
    "        np.array([0.5,  0.2, 0,0,0,0,0,0]), # S3 positivo\n",
    "        np.array([0.5, -0.2, 0,0,0,0,0,0]), # S3 negativo\n",
    "        # Semilla para FL -> 0 (Arregla los cortes en V de S9 vs S3)\n",
    "        np.array([0.01, 0.0, 0,0,0,0,0,0]), \n",
    "        # Semilla para FL -> 1 (Arregla la punta del triángulo AFB vs FL)\n",
    "        np.array([0.99, 0.0, 0,0,0,0,0,0]), \n",
    "    ]\n",
    "\n",
    "    for x_val in x_vals:\n",
    "        def objective_min(p): return p[y_idx]\n",
    "        def objective_max(p): return -p[y_idx]\n",
    "        def fix_x_constraint(p): return p[x_idx] - x_val\n",
    "        def phys_constraints(p): return constraints(p)\n",
    "        cons = [{'type': 'eq', 'fun': fix_x_constraint},{'type': 'ineq', 'fun': phys_constraints}]\n",
    "        best_min = np.inf\n",
    "        best_max = -np.inf\n",
    "        found_valid_min = False\n",
    "        found_valid_max = False\n",
    "        \n",
    "        for seed in base_guesses:\n",
    "            x0 = seed.copy()\n",
    "            x0[x_idx] = x_val \n",
    "            \n",
    "            # --- Buscar MÁXIMO ---\n",
    "            res_max = minimize(objective_max, x0, method='SLSQP', bounds=bounds, constraints=cons, tol=1e-4)\n",
    "            if res_max.success and np.min(constraints(res_max.x)) > -1e-5:\n",
    "                val = -res_max.fun\n",
    "                if val > best_max: \n",
    "                    best_max = val\n",
    "                    found_valid_max = True\n",
    "\n",
    "            # --- Buscar MÍNIMO ---\n",
    "            res_min = minimize(objective_min, x0, method='SLSQP', bounds=bounds, constraints=cons, tol=1e-4)\n",
    "            if res_min.success and np.min(constraints(res_min.x)) > -1e-5:\n",
    "                val = res_min.fun\n",
    "                if val < best_min: \n",
    "                    best_min = val\n",
    "                    found_valid_min = True\n",
    "\n",
    "        if found_valid_min and found_valid_max and best_max >= best_min:\n",
    "            valid_x.append(x_val)\n",
    "            y_max_vals.append(best_max)\n",
    "            y_min_vals.append(best_min)\n",
    "    \n",
    "    return np.array(valid_x), np.array(y_min_vals), np.array(y_max_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f164454",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from itertools import combinations\n",
    "\n",
    "def save_boundaries(n_steps=150):\n",
    "    params = ['FL', 'S3', 'S9', 'AFB', 'S4', 'S7', 'S5', 'S8']\n",
    "    pairs = list(combinations(enumerate(params), 2))\n",
    "    \n",
    "    # Crear directorios\n",
    "    os.makedirs(\"Plots/BoundaryData\", exist_ok=True)\n",
    "    \n",
    "    boundary_results = {}\n",
    "    print(f\"Iniciando cálculo de {len(pairs)} fronteras...\")\n",
    "    \n",
    "    for i, ((idx_x, name_x), (idx_y, name_y)) in enumerate(pairs):\n",
    "        start = time.time()\n",
    "        vx, v_min, v_max = get_profiled_boundary(idx_x, idx_y, n_steps=n_steps)\n",
    "        \n",
    "        # Guardamos en un diccionario usando un string único como llave\n",
    "        key = f\"{name_x}_vs_{name_y}\"\n",
    "        boundary_results[key] = {'x': vx, 'min': v_min, 'max': v_max}\n",
    "        \n",
    "        elapsed = time.time() - start\n",
    "        print(f\"[{i+1}/28] {key} calculado en {elapsed:.2f}s\")\n",
    "\n",
    "    # Guardar todo en un solo archivo comprimido\n",
    "    np.savez_compressed(\"Plots/BoundaryData/theoretical_boundaries.npz\", **boundary_results)\n",
    "    print(\"\\n¡Datos de fronteras guardados en Plots/BoundaryData/theoretical_boundaries.npz!\")\n",
    "\n",
    "save_boundaries(n_steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f877719",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mplhep as hep\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "import os\n",
    "hep.style.use(\"CMS\")\n",
    "\n",
    "def plot_from_saved_data(df_data):\n",
    "    data = np.load(\"Plots/BoundaryData/theoretical_boundaries.npz\", allow_pickle=True)\n",
    "    params = ['FL', 'S3', 'S9', 'AFB', 'S4', 'S7', 'S5', 'S8']\n",
    "    latex_labels = {'FL': r'$F_L$', 'S3': r'$S_3$', 'S9': r'$S_9$', 'AFB': r'$A_{FB}$', 'S4': r'$S_4$', 'S7': r'$S_7$', 'S5': r'$S_5$', 'S8': r'$S_8$'}\n",
    "    pairs = list(combinations(params, 2))\n",
    "    \n",
    "    output_dir = \"Plots/BoundaryData\"\n",
    "    os.makedirs(output_dir, exist_ok=True)    \n",
    "    print(\"Generando ploteos con estilo CMS...\")\n",
    "\n",
    "    for name_x, name_y in pairs:\n",
    "        key = f\"{name_x}_vs_{name_y}\"\n",
    "        if key not in data: continue\n",
    "        res = data[key].item()\n",
    "        vx, v_min, v_max = res['x'], res['min'], res['max']\n",
    "        \n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "        ax.scatter(df_data[name_x], df_data[name_y], s=1, alpha=1, c='b', label='Monte Carlo', rasterized=True, marker='o', linewidths=0)   \n",
    "        ax.plot(vx, v_max, color='red', linestyle='--', lw=1, label='Theoretical Limit')\n",
    "        ax.plot(vx, v_min, color='red', linestyle='--', lw=1)\n",
    "\n",
    "        ax.set_xlabel(latex_labels[name_x], fontsize=12)\n",
    "        ax.set_ylabel(latex_labels[name_y], fontsize=12)\n",
    "        ax.grid(True, linestyle=':', alpha=0.4)\n",
    "        if name_x == 'FL': ax.set_xlim(0, 1)\n",
    "        if name_y == 'FL': ax.set_ylim(0, 1)\n",
    "        \n",
    "        ax.legend(loc='best', fontsize=12, frameon=True, framealpha=0.8, edgecolor='white')\n",
    "        ax.tick_params(axis='both', which='major', labelsize=20, length=10, width=1)\n",
    "        ax.tick_params(axis='both', which='minor', length=5, width=1)\n",
    "\n",
    "        hep.cms.label(data=False, rlabel=\"Theoretical Phase Space\", loc=0, ax=ax)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/{key}_CMS.png\", dpi=150, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "\n",
    "    print(f\"Hecho. Gráficas CMS guardadas en: {output_dir}\")\n",
    "\n",
    "# Ejecución\n",
    "if 'df_phys_region' in locals():\n",
    "    plot_from_saved_data(df_phys_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6347281d",
   "metadata": {},
   "source": [
    "# Proyecciones V2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47c883d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zfit\n",
    "from zfit import z\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- REDEFINICIÓN: Eliminamos _integrate para permitir proyecciones numéricas ---\n",
    "class FullAngular_Physical_PDF(zfit.pdf.BasePDF):\n",
    "    def __init__(self, obs, FL, S3, S9, AFB, S4, S7, S5, S8, name=\"FullAngular_Physical_PDF\"):\n",
    "        params = {\n",
    "            'FL': FL, 'S3': S3, 'S9': S9, 'AFB': AFB,\n",
    "            'S4': S4, 'S7': S7, 'S5': S5, 'S8': S8\n",
    "        }\n",
    "        super().__init__(obs, params, name=name)\n",
    "    \n",
    "    def _unnormalized_pdf(self, x):\n",
    "        vars_list = z.unstack_x(x)\n",
    "        cos_l = vars_list[0]\n",
    "        cos_k = vars_list[1]\n",
    "        phi   = vars_list[2]\n",
    "        \n",
    "        sin_k = tf.sqrt(1.0 - cos_k**2)\n",
    "        sin_l = tf.sqrt(1.0 - cos_l**2)\n",
    "        sin2_k = sin_k**2\n",
    "        cos2_k = cos_k**2\n",
    "        sin2_l = sin_l**2\n",
    "        \n",
    "        cos2l_term = 2.0 * cos_l**2 - 1.0\n",
    "        sin2l_term = 2.0 * sin_l * cos_l\n",
    "        sin2k_term = 2.0 * sin_k * cos_k\n",
    "        \n",
    "        cos_phi = tf.cos(phi)\n",
    "        sin_phi = tf.sin(phi)\n",
    "        cos2_phi = tf.cos(2.0 * phi)\n",
    "        sin2_phi = tf.sin(2.0 * phi)\n",
    "\n",
    "        FL = self.params['FL']\n",
    "        S3 = self.params['S3']\n",
    "        S9 = self.params['S9']\n",
    "        AFB = self.params['AFB']\n",
    "        S4 = self.params['S4']\n",
    "        S7 = self.params['S7']\n",
    "        S5 = self.params['S5']\n",
    "        S8 = self.params['S8']\n",
    "        \n",
    "        term1 = 0.75 * (1.0 - FL) * sin2_k\n",
    "        term2 = FL * cos2_k\n",
    "        term3 = 0.25 * (1.0 - FL) * sin2_k * cos2l_term\n",
    "        term4 = -1.0 * FL * cos2_k * cos2l_term\n",
    "        term5 = S3 * sin2_k * sin2_l * cos2_phi\n",
    "        term6 = S4 * sin2k_term * sin2l_term * cos_phi\n",
    "        term7 = S5 * sin2k_term * sin_l * cos_phi\n",
    "        term8 = (4.0/3.0) * AFB * sin2_k * cos_l\n",
    "        term9 = S7 * sin2k_term * sin_l * sin_phi\n",
    "        term10 = S8 * sin2k_term * sin2l_term * sin_phi\n",
    "        term11 = S9 * sin2_k * sin2_l * sin2_phi\n",
    "        \n",
    "        pdf = term1 + term2 + term3 + term4 + term5 + term6 + term7 + term8 + term9 + term10 + term11\n",
    "        return pdf\n",
    "\n",
    "    # NOTA: Se ha eliminado _integrate explícitamente.\n",
    "    # zfit ahora usará integración numérica por defecto, lo que permite\n",
    "    # calcular proyecciones (integrales parciales) correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78426651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Instanciando PDF física para proyecciones...\n",
      "--- Procesando plot CMS para: cosThetaL ---\n",
      "Estimated integral error ( 4.7820036572242623e-06 ) larger than tolerance ( 3e-06 ), which is maybe not enough (but maybe it's also fine). You can (best solution) implement an anatytical integral (see examples in repo) or manually set a higher number on the PDF with 'update_integration_options' and increase the 'max_draws' (or adjust 'tol'). If partial integration is chosen, this can lead to large memory consumption.This is a new warning checking the integral accuracy. It may warns too often as it is Work In Progress. If you have any observation on it, please tell us about it: https://github.com/zfit/zfit/issues/new/chooseTo suppress this warning, use zfit.settings.set_verbosity(-1).\n",
      "Estimated integral error ( 2.3815317593743481e-05 ) larger than tolerance ( 3e-06 ), which is maybe not enough (but maybe it's also fine). You can (best solution) implement an anatytical integral (see examples in repo) or manually set a higher number on the PDF with 'update_integration_options' and increase the 'max_draws' (or adjust 'tol'). If partial integration is chosen, this can lead to large memory consumption.This is a new warning checking the integral accuracy. It may warns too often as it is Work In Progress. If you have any observation on it, please tell us about it: https://github.com/zfit/zfit/issues/new/chooseTo suppress this warning, use zfit.settings.set_verbosity(-1).\n",
      "Estimated integral error ( 4.782862163937605e-06 ) larger than tolerance ( 3e-06 ), which is maybe not enough (but maybe it's also fine). You can (best solution) implement an anatytical integral (see examples in repo) or manually set a higher number on the PDF with 'update_integration_options' and increase the 'max_draws' (or adjust 'tol'). If partial integration is chosen, this can lead to large memory consumption.This is a new warning checking the integral accuracy. It may warns too often as it is Work In Progress. If you have any observation on it, please tell us about it: https://github.com/zfit/zfit/issues/new/chooseTo suppress this warning, use zfit.settings.set_verbosity(-1).\n",
      "Estimated integral error ( 2.3815317593743481e-05 ) larger than tolerance ( 3e-06 ), which is maybe not enough (but maybe it's also fine). You can (best solution) implement an anatytical integral (see examples in repo) or manually set a higher number on the PDF with 'update_integration_options' and increase the 'max_draws' (or adjust 'tol'). If partial integration is chosen, this can lead to large memory consumption.This is a new warning checking the integral accuracy. It may warns too often as it is Work In Progress. If you have any observation on it, please tell us about it: https://github.com/zfit/zfit/issues/new/chooseTo suppress this warning, use zfit.settings.set_verbosity(-1).\n",
      "Estimated integral error ( 4.7826891768868406e-06 ) larger than tolerance ( 3e-06 ), which is maybe not enough (but maybe it's also fine). You can (best solution) implement an anatytical integral (see examples in repo) or manually set a higher number on the PDF with 'update_integration_options' and increase the 'max_draws' (or adjust 'tol'). If partial integration is chosen, this can lead to large memory consumption.This is a new warning checking the integral accuracy. It may warns too often as it is Work In Progress. If you have any observation on it, please tell us about it: https://github.com/zfit/zfit/issues/new/chooseTo suppress this warning, use zfit.settings.set_verbosity(-1).\n",
      "Estimated integral error ( 2.3815317593743481e-05 ) larger than tolerance ( 3e-06 ), which is maybe not enough (but maybe it's also fine). You can (best solution) implement an anatytical integral (see examples in repo) or manually set a higher number on the PDF with 'update_integration_options' and increase the 'max_draws' (or adjust 'tol'). If partial integration is chosen, this can lead to large memory consumption.This is a new warning checking the integral accuracy. It may warns too often as it is Work In Progress. If you have any observation on it, please tell us about it: https://github.com/zfit/zfit/issues/new/chooseTo suppress this warning, use zfit.settings.set_verbosity(-1).\n",
      "Estimated integral error ( 4.782862163937605e-06 ) larger than tolerance ( 3e-06 ), which is maybe not enough (but maybe it's also fine). You can (best solution) implement an anatytical integral (see examples in repo) or manually set a higher number on the PDF with 'update_integration_options' and increase the 'max_draws' (or adjust 'tol'). If partial integration is chosen, this can lead to large memory consumption.This is a new warning checking the integral accuracy. It may warns too often as it is Work In Progress. If you have any observation on it, please tell us about it: https://github.com/zfit/zfit/issues/new/chooseTo suppress this warning, use zfit.settings.set_verbosity(-1).\n",
      "Estimated integral error ( 2.3815317593743481e-05 ) larger than tolerance ( 3e-06 ), which is maybe not enough (but maybe it's also fine). You can (best solution) implement an anatytical integral (see examples in repo) or manually set a higher number on the PDF with 'update_integration_options' and increase the 'max_draws' (or adjust 'tol'). If partial integration is chosen, this can lead to large memory consumption.This is a new warning checking the integral accuracy. It may warns too often as it is Work In Progress. If you have any observation on it, please tell us about it: https://github.com/zfit/zfit/issues/new/chooseTo suppress this warning, use zfit.settings.set_verbosity(-1).\n",
      "    -> Gráfica guardada: Plots/Analytic_Projections_CMS/CMS_Proj_cosThetaL.png\n",
      "--- Procesando plot CMS para: cosThetaK ---\n",
      "Estimated integral error ( 4.8046117784449048e-06 ) larger than tolerance ( 3e-06 ), which is maybe not enough (but maybe it's also fine). You can (best solution) implement an anatytical integral (see examples in repo) or manually set a higher number on the PDF with 'update_integration_options' and increase the 'max_draws' (or adjust 'tol'). If partial integration is chosen, this can lead to large memory consumption.This is a new warning checking the integral accuracy. It may warns too often as it is Work In Progress. If you have any observation on it, please tell us about it: https://github.com/zfit/zfit/issues/new/chooseTo suppress this warning, use zfit.settings.set_verbosity(-1).\n",
      "Estimated integral error ( 2.3815317593743481e-05 ) larger than tolerance ( 3e-06 ), which is maybe not enough (but maybe it's also fine). You can (best solution) implement an anatytical integral (see examples in repo) or manually set a higher number on the PDF with 'update_integration_options' and increase the 'max_draws' (or adjust 'tol'). If partial integration is chosen, this can lead to large memory consumption.This is a new warning checking the integral accuracy. It may warns too often as it is Work In Progress. If you have any observation on it, please tell us about it: https://github.com/zfit/zfit/issues/new/chooseTo suppress this warning, use zfit.settings.set_verbosity(-1).\n",
      "Estimated integral error ( 4.791965057808375e-06 ) larger than tolerance ( 3e-06 ), which is maybe not enough (but maybe it's also fine). You can (best solution) implement an anatytical integral (see examples in repo) or manually set a higher number on the PDF with 'update_integration_options' and increase the 'max_draws' (or adjust 'tol'). If partial integration is chosen, this can lead to large memory consumption.This is a new warning checking the integral accuracy. It may warns too often as it is Work In Progress. If you have any observation on it, please tell us about it: https://github.com/zfit/zfit/issues/new/chooseTo suppress this warning, use zfit.settings.set_verbosity(-1).\n",
      "Estimated integral error ( 2.3815317593743481e-05 ) larger than tolerance ( 3e-06 ), which is maybe not enough (but maybe it's also fine). You can (best solution) implement an anatytical integral (see examples in repo) or manually set a higher number on the PDF with 'update_integration_options' and increase the 'max_draws' (or adjust 'tol'). If partial integration is chosen, this can lead to large memory consumption.This is a new warning checking the integral accuracy. It may warns too often as it is Work In Progress. If you have any observation on it, please tell us about it: https://github.com/zfit/zfit/issues/new/chooseTo suppress this warning, use zfit.settings.set_verbosity(-1).\n",
      "Estimated integral error ( 4.7716899659413877e-06 ) larger than tolerance ( 3e-06 ), which is maybe not enough (but maybe it's also fine). You can (best solution) implement an anatytical integral (see examples in repo) or manually set a higher number on the PDF with 'update_integration_options' and increase the 'max_draws' (or adjust 'tol'). If partial integration is chosen, this can lead to large memory consumption.This is a new warning checking the integral accuracy. It may warns too often as it is Work In Progress. If you have any observation on it, please tell us about it: https://github.com/zfit/zfit/issues/new/chooseTo suppress this warning, use zfit.settings.set_verbosity(-1).\n",
      "Estimated integral error ( 2.3815317593743481e-05 ) larger than tolerance ( 3e-06 ), which is maybe not enough (but maybe it's also fine). You can (best solution) implement an anatytical integral (see examples in repo) or manually set a higher number on the PDF with 'update_integration_options' and increase the 'max_draws' (or adjust 'tol'). If partial integration is chosen, this can lead to large memory consumption.This is a new warning checking the integral accuracy. It may warns too often as it is Work In Progress. If you have any observation on it, please tell us about it: https://github.com/zfit/zfit/issues/new/chooseTo suppress this warning, use zfit.settings.set_verbosity(-1).\n",
      "Estimated integral error ( 4.791965057808375e-06 ) larger than tolerance ( 3e-06 ), which is maybe not enough (but maybe it's also fine). You can (best solution) implement an anatytical integral (see examples in repo) or manually set a higher number on the PDF with 'update_integration_options' and increase the 'max_draws' (or adjust 'tol'). If partial integration is chosen, this can lead to large memory consumption.This is a new warning checking the integral accuracy. It may warns too often as it is Work In Progress. If you have any observation on it, please tell us about it: https://github.com/zfit/zfit/issues/new/chooseTo suppress this warning, use zfit.settings.set_verbosity(-1).\n",
      "Estimated integral error ( 2.3815317593743481e-05 ) larger than tolerance ( 3e-06 ), which is maybe not enough (but maybe it's also fine). You can (best solution) implement an anatytical integral (see examples in repo) or manually set a higher number on the PDF with 'update_integration_options' and increase the 'max_draws' (or adjust 'tol'). If partial integration is chosen, this can lead to large memory consumption.This is a new warning checking the integral accuracy. It may warns too often as it is Work In Progress. If you have any observation on it, please tell us about it: https://github.com/zfit/zfit/issues/new/chooseTo suppress this warning, use zfit.settings.set_verbosity(-1).\n",
      "    -> Gráfica guardada: Plots/Analytic_Projections_CMS/CMS_Proj_cosThetaK.png\n",
      "--- Procesando plot CMS para: phi ---\n",
      "Estimated integral error ( 4.7837541500399882e-06 ) larger than tolerance ( 3e-06 ), which is maybe not enough (but maybe it's also fine). You can (best solution) implement an anatytical integral (see examples in repo) or manually set a higher number on the PDF with 'update_integration_options' and increase the 'max_draws' (or adjust 'tol'). If partial integration is chosen, this can lead to large memory consumption.This is a new warning checking the integral accuracy. It may warns too often as it is Work In Progress. If you have any observation on it, please tell us about it: https://github.com/zfit/zfit/issues/new/chooseTo suppress this warning, use zfit.settings.set_verbosity(-1).\n",
      "Estimated integral error ( 2.3815317593743481e-05 ) larger than tolerance ( 3e-06 ), which is maybe not enough (but maybe it's also fine). You can (best solution) implement an anatytical integral (see examples in repo) or manually set a higher number on the PDF with 'update_integration_options' and increase the 'max_draws' (or adjust 'tol'). If partial integration is chosen, this can lead to large memory consumption.This is a new warning checking the integral accuracy. It may warns too often as it is Work In Progress. If you have any observation on it, please tell us about it: https://github.com/zfit/zfit/issues/new/chooseTo suppress this warning, use zfit.settings.set_verbosity(-1).\n",
      "Estimated integral error ( 4.7836655102040825e-06 ) larger than tolerance ( 3e-06 ), which is maybe not enough (but maybe it's also fine). You can (best solution) implement an anatytical integral (see examples in repo) or manually set a higher number on the PDF with 'update_integration_options' and increase the 'max_draws' (or adjust 'tol'). If partial integration is chosen, this can lead to large memory consumption.This is a new warning checking the integral accuracy. It may warns too often as it is Work In Progress. If you have any observation on it, please tell us about it: https://github.com/zfit/zfit/issues/new/chooseTo suppress this warning, use zfit.settings.set_verbosity(-1).\n",
      "Estimated integral error ( 2.3815317593743481e-05 ) larger than tolerance ( 3e-06 ), which is maybe not enough (but maybe it's also fine). You can (best solution) implement an anatytical integral (see examples in repo) or manually set a higher number on the PDF with 'update_integration_options' and increase the 'max_draws' (or adjust 'tol'). If partial integration is chosen, this can lead to large memory consumption.This is a new warning checking the integral accuracy. It may warns too often as it is Work In Progress. If you have any observation on it, please tell us about it: https://github.com/zfit/zfit/issues/new/chooseTo suppress this warning, use zfit.settings.set_verbosity(-1).\n",
      "Estimated integral error ( 4.7835994286037933e-06 ) larger than tolerance ( 3e-06 ), which is maybe not enough (but maybe it's also fine). You can (best solution) implement an anatytical integral (see examples in repo) or manually set a higher number on the PDF with 'update_integration_options' and increase the 'max_draws' (or adjust 'tol'). If partial integration is chosen, this can lead to large memory consumption.This is a new warning checking the integral accuracy. It may warns too often as it is Work In Progress. If you have any observation on it, please tell us about it: https://github.com/zfit/zfit/issues/new/chooseTo suppress this warning, use zfit.settings.set_verbosity(-1).\n",
      "Estimated integral error ( 2.3815317593743481e-05 ) larger than tolerance ( 3e-06 ), which is maybe not enough (but maybe it's also fine). You can (best solution) implement an anatytical integral (see examples in repo) or manually set a higher number on the PDF with 'update_integration_options' and increase the 'max_draws' (or adjust 'tol'). If partial integration is chosen, this can lead to large memory consumption.This is a new warning checking the integral accuracy. It may warns too often as it is Work In Progress. If you have any observation on it, please tell us about it: https://github.com/zfit/zfit/issues/new/chooseTo suppress this warning, use zfit.settings.set_verbosity(-1).\n",
      "Estimated integral error ( 4.7836655102040825e-06 ) larger than tolerance ( 3e-06 ), which is maybe not enough (but maybe it's also fine). You can (best solution) implement an anatytical integral (see examples in repo) or manually set a higher number on the PDF with 'update_integration_options' and increase the 'max_draws' (or adjust 'tol'). If partial integration is chosen, this can lead to large memory consumption.This is a new warning checking the integral accuracy. It may warns too often as it is Work In Progress. If you have any observation on it, please tell us about it: https://github.com/zfit/zfit/issues/new/chooseTo suppress this warning, use zfit.settings.set_verbosity(-1).\n",
      "Estimated integral error ( 2.3815317593743481e-05 ) larger than tolerance ( 3e-06 ), which is maybe not enough (but maybe it's also fine). You can (best solution) implement an anatytical integral (see examples in repo) or manually set a higher number on the PDF with 'update_integration_options' and increase the 'max_draws' (or adjust 'tol'). If partial integration is chosen, this can lead to large memory consumption.This is a new warning checking the integral accuracy. It may warns too often as it is Work In Progress. If you have any observation on it, please tell us about it: https://github.com/zfit/zfit/issues/new/chooseTo suppress this warning, use zfit.settings.set_verbosity(-1).\n",
      "    -> Gráfica guardada: Plots/Analytic_Projections_CMS/CMS_Proj_phi.png\n",
      ">>> Todas las proyecciones estilo CMS generadas.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import zfit\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from scipy.stats import chi2\n",
    "import mplhep as hep # Asegúrate de tener instalado mplhep (pip install mplhep)\n",
    "\n",
    "# Configuración de estilo CMS si lo deseas (opcional)\n",
    "# hep.style.use(\"CMS\")\n",
    "\n",
    "def plot_analytical_projections_cms(sampler, phys_params_dict, folder_out=\"Plots/Analytic_Projections_CMS\", n_bins=30):\n",
    "    \"\"\"\n",
    "    Genera proyecciones con estilo CMS:\n",
    "    - Panel superior: Datos vs Fit + Caja de texto con parámetros y Chi2.\n",
    "    - Panel inferior: Pulls (Residuales normalizados).\n",
    "    \"\"\"\n",
    "    if not os.path.exists(folder_out):\n",
    "        os.makedirs(folder_out)\n",
    "\n",
    "    obs_space = sampler.space\n",
    "    observables = obs_space.obs  \n",
    "    \n",
    "    # 1. Instanciar la PDF Física con los parámetros del Best Fit\n",
    "    print(f\">>> Instanciando PDF física para proyecciones...\")\n",
    "    pdf_model = FullAngular_Physical_PDF(\n",
    "        obs=obs_space,\n",
    "        **phys_params_dict \n",
    "    )\n",
    "\n",
    "    # Etiquetas LaTeX para los ejes\n",
    "    latex_labels = {\n",
    "        observables[0]: r'$\\cos(\\theta_l)$',\n",
    "        observables[1]: r'$\\cos(\\theta_K)$',\n",
    "        observables[2]: r'$\\phi$'\n",
    "    }\n",
    "\n",
    "    # Total de eventos (para escalar la PDF)\n",
    "    n_total = sampler.n_events.numpy()\n",
    "\n",
    "    for obs_name in observables:\n",
    "        print(f\"--- Procesando plot CMS para: {obs_name} ---\")\n",
    "        \n",
    "        # A. Crear la PDF Proyectada (integrando las otras variables)\n",
    "        vars_to_integrate = [var for var in observables if var != obs_name]\n",
    "        limits_integration = obs_space.with_obs(vars_to_integrate)\n",
    "        pdf_proj = pdf_model.create_projection_pdf(limits=limits_integration)\n",
    "        \n",
    "        # B. Preparar Datos (Histograma)\n",
    "        idx = observables.index(obs_name)\n",
    "        data_column = sampler.unstack_x()[idx] # Corrección de extracción de lista\n",
    "        \n",
    "        if hasattr(data_column, \"numpy\"):\n",
    "            data_np = data_column.numpy()\n",
    "        else:\n",
    "            data_np = np.array(data_column)\n",
    "            \n",
    "        counts, bin_edges = np.histogram(data_np, bins=n_bins)\n",
    "        bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "        bin_width = bin_edges[1] - bin_edges[0]\n",
    "        y_err_data = np.sqrt(counts)\n",
    "        \n",
    "        # C. Calcular Modelo (Curva Suave y Puntos para Chi2)\n",
    "        lower, upper = obs_space.with_obs(obs_name).limit1d\n",
    "        \n",
    "        # C.1 Para el plot suave\n",
    "        x_fine = np.linspace(lower, upper, 400)\n",
    "        y_smooth_density = pdf_proj.pdf(x_fine).numpy()\n",
    "        y_smooth_counts = y_smooth_density * n_total * bin_width\n",
    "        \n",
    "        # C.2 Para Chi2 y Pulls (evaluado en el centro del bin)\n",
    "        y_model_at_centers_density = pdf_proj.pdf(bin_centers).numpy()\n",
    "        y_model_at_centers = y_model_at_centers_density * n_total * bin_width\n",
    "        \n",
    "        # D. Calcular Pulls y Chi2\n",
    "        # Pull = (Data - Model) / Error_Data\n",
    "        # Evitamos división por cero donde counts es 0\n",
    "        mask = counts > 0\n",
    "        pulls = np.zeros_like(counts, dtype=float)\n",
    "        pulls[mask] = (counts[mask] - y_model_at_centers[mask]) / y_err_data[mask]\n",
    "        \n",
    "        chi2_val = np.sum(pulls[mask]**2)\n",
    "        ndof = n_bins - 1 # Aproximación para proyección 1D\n",
    "        chi2_red = chi2_val / ndof\n",
    "        p_val = 1 - chi2.cdf(chi2_val, ndof)\n",
    "\n",
    "        # ====== PLOTTING ESTILO CMS ======\n",
    "        fig = plt.figure(figsize=(9, 9))\n",
    "        gs = GridSpec(2, 1, height_ratios=[3.5, 1], hspace=0.08)\n",
    "        ax0 = fig.add_subplot(gs[0])\n",
    "        ax1 = fig.add_subplot(gs[1])\n",
    "\n",
    "        # --- Panel Superior (Datos + Modelo) ---\n",
    "        ax0.errorbar(bin_centers, counts, yerr=y_err_data, xerr=bin_width/2.0, \n",
    "                     fmt='ks', markersize=4, elinewidth=1, capsize=2, label='Toy (data)')\n",
    "\n",
    "        ax0.plot(x_fine, y_smooth_counts, '-', linewidth=2, label='Analytical PDF', color='b')\n",
    "        \n",
    "        # Límites dinámicos con margen superior\n",
    "        max_data = np.max(counts + y_err_data)\n",
    "        max_model = np.max(y_smooth_counts)\n",
    "        y_max_plot = max(max_data, max_model)\n",
    "        ax0.set_ylim(0, y_max_plot * 1.5)\n",
    "        \n",
    "        ax0.set_xlim(lower, upper)\n",
    "        ax0.set_xticklabels([]) # Ocultar etiquetas X del panel superior\n",
    "        ax0.set_ylabel(f'Events / {bin_width:.2f}', fontsize=16)\n",
    "        ax0.grid(True, alpha=0.2)\n",
    "        ax0.legend(loc='upper right', fontsize=13, frameon=False, bbox_to_anchor=(0.98, 0.88))\n",
    "\n",
    "        # Etiqueta CMS\n",
    "        # Nota: Si falla hep.cms.label, comenta esta línea.\n",
    "        try:\n",
    "            hep.cms.label(data=False, label=\"Simulation\", loc=0, ax=ax0, rlabel=\"13 TeV\")\n",
    "        except:\n",
    "            ax0.text(0.05, 1.01, \"CMS Simulation (Preliminary)\", transform=ax0.transAxes, fontsize=14, fontweight='bold')\n",
    "\n",
    "        # --- Caja de Texto con Estadísticas ---\n",
    "        # Extraemos valores del diccionario\n",
    "        FL_val = phys_params_dict.get('FL', 0)\n",
    "        AFB_val = phys_params_dict.get('AFB', 0)\n",
    "        S3_val = phys_params_dict.get('S3', 0)\n",
    "        S9_val = phys_params_dict.get('S9', 0)\n",
    "\n",
    "        param_text = (\n",
    "            rf'$F_L = {FL_val:.3f}$' + '\\n' +\n",
    "            rf'$A_{{FB}} = {AFB_val:.3f}$' + '\\n' +\n",
    "            rf'$S_3 = {S3_val:.3f}$' + '\\n' +\n",
    "            rf'$S_9 = {S9_val:.3f}$' + '\\n' +\n",
    "            r'----------------' + '\\n' + \n",
    "            rf'$\\chi^2/\\mathrm{{ndof}} = {chi2_val:.1f}/{ndof} = {chi2_red:.2f}$' + '\\n' +\n",
    "            rf'$p$-value = {p_val:.3f}'\n",
    "        )\n",
    "\n",
    "        ax0.text(0.05, 0.92, param_text, transform=ax0.transAxes, fontsize=12, \n",
    "                 verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.9, edgecolor='#dddddd'))\n",
    "\n",
    "        # --- Panel Inferior (Pulls) ---\n",
    "        ax1.errorbar(bin_centers, pulls, \n",
    "                     yerr=1.0,           \n",
    "                     xerr=0,             \n",
    "                     fmt='ks',           \n",
    "                     markersize=3,       \n",
    "                     elinewidth=1.0,     \n",
    "                     capsize=0)          \n",
    "        \n",
    "        # Líneas de referencia\n",
    "        ax1.axhline(0, color='black', linewidth=1.0, linestyle='-')\n",
    "        ax1.axhline(3, color='gray', linestyle=':', linewidth=1, alpha=0.8) # 3 sigma\n",
    "        ax1.axhline(-3, color='gray', linestyle=':', linewidth=1, alpha=0.8)\n",
    "        \n",
    "        # Relleno de bandas sigma (opcional, estilo ROOT)\n",
    "        ax1.fill_between([lower, upper], -1, 1, color='yellow', alpha=0.1) # 1 sigma\n",
    "        ax1.fill_between([lower, upper], -2, 2, color='green', alpha=0.05) # 2 sigma\n",
    "\n",
    "        xlabel = latex_labels.get(obs_name, obs_name)\n",
    "        ax1.set_xlabel(xlabel, fontsize=16)\n",
    "        ax1.set_ylabel(r'Pull $(\\sigma)$', fontsize=13)\n",
    "        ax1.set_xlim(lower, upper)\n",
    "        ax1.set_ylim(-4.9, 4.9)\n",
    "        ax1.grid(True, alpha=0.2)\n",
    "\n",
    "        # Ajuste de ticks\n",
    "        ax0.tick_params(axis='both', which='major', labelsize=14) \n",
    "        ax1.tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "        # Guardar\n",
    "        outname = os.path.join(folder_out, f\"CMS_Proj_{obs_name}.png\")\n",
    "        plt.savefig(outname, bbox_inches='tight', dpi=200)\n",
    "        plt.close(fig)\n",
    "        print(f\"    -> Gráfica guardada: {outname}\")\n",
    "\n",
    "    print(\">>> Todas las proyecciones estilo CMS generadas.\")\n",
    "\n",
    "# --- LLAMADA A LA FUNCIÓN ---\n",
    "plot_analytical_projections_cms(\n",
    "    sampler=sampler,\n",
    "    phys_params_dict=best_fit_phys_dict,\n",
    "    folder_out=\"Plots/Analytic_Projections_CMS\",\n",
    "    n_bins=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5fb015",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "haza_wokr_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
